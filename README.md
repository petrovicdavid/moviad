<p align="center">
<img src="https://github.com/user-attachments/assets/93cf614e-2d18-4aa3-b670-80206c03bb7c" alt="drawing" style="width:300px;"/>
</p>

## Project Objective

MoViAD is an Open Source library for easy and modular Visual Anomaly Detection, built for industrial and research purposes. 
The library contains some State-of-the-Art models with their trainers, standard datasets, evaluator for calculating standard metrics, and a feature extractor.
The library structure is totally modular, allowing an easy isolation of the components needed for your project. 
The library will support different scenarios (see also below):
* Unsupervised anomaly detection âœ…
* Contaminated anomaly detection âœ…
* Semi-supervised anomaly detection ðŸš§ (Work in progress)
* Few shot anomaly detection ðŸš§ (Work in progress)
* Continual anomaly detection ðŸš§ (Work in progress)


## How to use the library

The library follows this structure:
- inside the <code>/models/model_name</code> directory is present the code of anomaly detection models
- inside the <code>/trainers</code> directory is present the code for training the anomaly detection models
- inside the <code>/datasets</code> directory is present the code for the anomaly detection datasets that must be used
- inside the <code>/utilies</code> directory is present the code for anomaly detection utilities

## Execution example

Inside the <code>/main_scripts</code> directory are present some execution scripts for training and testing the AD models. 

For example, for training patchcore: 

```bash
python main_scripts/main_patchcore.py --mode train --dataset_path /home/datasets/mvtec --category pill --backbone mobilenet_v2 --ad_layers features.4 features.7 features.10 --device cuda:0 --save_path ./patch.pt 
```

For every main script all its parameters are documented. 

**AD Models**

- PatchCore: [paper](https://openaccess.thecvf.com/content/CVPR2022/html/Roth_Towards_Total_Recall_in_Industrial_Anomaly_Detection_CVPR_2022_paper.html), [code](https://github.com/amazon-science/patchcore-inspection)
- CFA [paper](https://ieeexplore.ieee.org/abstract/document/9839549), [code](https://github.com/sungwool/CFA_for_anomaly_localization)
- Student-Teacher Feature Pyramid: [paper](https://arxiv.org/abs/2103.04257), [code](https://github.com/gdwang08/STFPM)
- PaSTe: [paper](https://arxiv.org/abs/2103.04257)
- PaDiM: [paper with code](https://paperswithcode.com/paper/padim-a-patch-distribution-modeling-framework)
- FastFlow: [paper](https://arxiv.org/abs/2111.07677)
- GANomaly: [paper](https://arxiv.org/abs/1805.06725)
- SuperSimpleNet: [paper](https://arxiv.org/abs/2408.03143)
- RD4AD: [paper](https://arxiv.org/abs/2201.10703)

**Feature Extraction Backbones**

- MobileNet: [link](https://paperswithcode.com/paper/mobilenets-efficient-convolutional-neural)
- PhiNet: [link](https://paperswithcode.com/paper/phinets-a-scalable-backbone-for-low-power-ai)
- MicroNet: [link](https://paperswithcode.com/paper/micronet-improving-image-recognition-with)
- MCUNet: [link](https://paperswithcode.com/paper/mcunet-tiny-deep-learning-on-iot-devices)
- WideResnet

**Datasets**

- MVTecAD: [link](https://paperswithcode.com/dataset/mvtecad)
- VisA: [link](https://paperswithcode.com/dataset/visa)
- RealIAD: [paper](https://arxiv.org/abs/2403.12580)
- MIIC Dataset: [link](https://github.com/wenbihan/MIIC-IAD) 

**Scenarios**

- Unsupervised: standard VAD scenario where the model is trained on a dataset of normal samples and evaluated on a dataset of normal and abnormal samples.
- Contaminated: model trained on a dataset of both normal and abnormal samples, a certain percentage of abnormal samples (train set contamination).
- Semi-supervised: model trained on a dataset of mostly normal samples with a small percentage of labeled abnormal samples, leveraging both types of data to improve anomaly detection performance.
- Few-shot: model trained on a dataset of normal samples and a few abnormal samples which are labeled.
- Continual: the model is trained on a specific subset, called task, and then updated with new tasks. The model should be able to perform well on all seen tasks.

## How to Install

Inside the main repository directory, after installing [uv](https://docs.astral.sh/uv/getting-started/installation/):

```bash
uv sync
```

## Contribute

If you want to contribute to the repository, follow the present code structure: 
- inside the <code>/models/model_name</code> directory put the code for possible new anomaly detection models
- inside the <code>/trainers</code> directory put the code for training an anomaly detection model
- inside the <code>/datasets</code> directory put the code for possible new anomaly detection datasets that must be used

Every contribution must be open with a pull request. 

**Execute tests**

Please, before opening a pull request, execute the tests with the command, and add new tests if you added new functionalities:

```bash
uv run pytest -v -s
```

**Build Documentation**
To build the documentation locally, use the following commands:

```bash
cd code-files/moviad/docs

uv run sphinx-build -b html . _build/html
```


## Citations

If you use MoViAD in your work, please cite us! ðŸ¤—

Works that use MoViAD: 
- "PaSTe: Improving the efficiency of visual anomaly detection at the edge", Manuel Barusco, Francesco Borsatti, Davide Dalle Pezze, Francesco Paissan, Elisabetta Farella, Gian Antonio Susto. [paper](https://arxiv.org/abs/2103.04257)
- "From Vision to Sound: Advancing Audio Anomaly Detection with Vision-Based Algorithm", Manuel Barusco, Francesco Borsatti, Davide Dalle Pezze, Francesco Paissan, Elisabetta Farella, Gian Antonio Susto. [paper](https://arxiv.org/pdf/2502.18328?)

